# Kafka KRaft Mode Deployment (No Zookeeper Required)
# Kafka Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kafka
  namespace: api-database
  labels:
    app: kafka
    component: message-broker
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kafka
  template:
    metadata:
      labels:
        app: kafka
        component: message-broker
    spec:
      containers:
      - name: kafka
        image: confluentinc/cp-kafka:7.4.0
        ports:
        - containerPort: 9092
          name: kafka
        - containerPort: 9093
          name: kafka-internal
        env:
        # KRaft Mode Configuration 
        - name: KAFKA_NODE_ID
          value: "1"
        - name: KAFKA_PROCESS_ROLES
          value: "broker,controller"
        - name: KAFKA_CONTROLLER_QUORUM_VOTERS
          value: "1@kafka-service:9093"
        - name: KAFKA_CONTROLLER_LISTENER_NAMES
          value: "CONTROLLER"
        - name: KAFKA_LISTENERS
          value: "PLAINTEXT://0.0.0.0:9092,CONTROLLER://0.0.0.0:9093"
        - name: KAFKA_ADVERTISED_LISTENERS
          value: "PLAINTEXT://kafka-service:9092"
        - name: KAFKA_LISTENER_SECURITY_PROTOCOL_MAP
          value: "CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT"
        - name: KAFKA_INTER_BROKER_LISTENER_NAME
          value: "PLAINTEXT"
        # Topic and Log Configuration
        - name: KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR
          value: "1"
        - name: KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR
          value: "1"
        - name: KAFKA_TRANSACTION_STATE_LOG_MIN_ISR
          value: "1"
        - name: KAFKA_AUTO_CREATE_TOPICS_ENABLE
          value: "true"
        - name: KAFKA_NUM_PARTITIONS
          value: "3"
        - name: KAFKA_DEFAULT_REPLICATION_FACTOR
          value: "1"
        - name: KAFKA_LOG_RETENTION_HOURS
          value: "168"
        - name: KAFKA_LOG_SEGMENT_BYTES
          value: "1073741824"
        - name: KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS
          value: "300000"
        # KRaft Metadata Configuration
        - name: KAFKA_METADATA_LOG_SEGMENT_BYTES
          value: "1073741824"
        - name: KAFKA_METADATA_MAX_RETENTION_BYTES
          value: "1073741824"
        volumeMounts:
        - name: kafka-data
          mountPath: /var/lib/kafka/data
        - name: kafka-logs
          mountPath: /var/lib/kafka/logs
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "1Gi"
            cpu: "1000m"
        livenessProbe:
          tcpSocket:
            port: 9092
          initialDelaySeconds: 60
          periodSeconds: 10
        readinessProbe:
          tcpSocket:
            port: 9092
          initialDelaySeconds: 30
          periodSeconds: 5
      volumes:
      - name: kafka-data
        persistentVolumeClaim:
          claimName: kafka-pvc
      - name: kafka-logs
        emptyDir: {}
---
# Kafka Internal Service (ClusterIP)
apiVersion: v1
kind: Service
metadata:
  name: kafka-service
  namespace: api-database
  labels:
    app: kafka
    component: message-broker
    service: kafka-internal
  annotations:
    description: "Internal Kafka service for cluster communication"
    prometheus.io/scrape: "true"
    prometheus.io/port: "9092"
spec:
  type: ClusterIP
  ports:
  - port: 9092
    targetPort: 9092
    protocol: TCP
    name: kafka
  selector:
    app: kafka
---
# Kafka External Service (NodePort)
apiVersion: v1
kind: Service
metadata:
  name: kafka-external
  namespace: api-database
  labels:
    app: kafka
    component: message-broker
    service: kafka-external
  annotations:
    description: "External Kafka service for development access"
    prometheus.io/scrape: "true"
    prometheus.io/port: "9092"
spec:
  type: NodePort
  ports:
  - port: 9092
    targetPort: 9092
    nodePort: 30092
    protocol: TCP
    name: kafka
  selector:
    app: kafka
---
# Kafka Persistent Volume Claim
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: kafka-pvc
  namespace: api-database
  labels:
    app: kafka
    component: message-broker
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
  storageClassName: "" # Uses default storage class
---
# Kafka ConfigMap for additional configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: kafka-config
  namespace: api-database
  labels:
    app: kafka
    component: message-broker
data:
  server.properties: |
    # Kafka KRaft Mode Configuration (No Zookeeper)
    node.id=1
    process.roles=broker,controller
    controller.quorum.voters=1@kafka-service:9093
    controller.listener.names=CONTROLLER
    listeners=PLAINTEXT://0.0.0.0:9092,CONTROLLER://0.0.0.0:9093
    advertised.listeners=PLAINTEXT://kafka-service:9092
    listener.security.protocol.map=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT
    inter.broker.listener.name=PLAINTEXT
    
    # Log configuration
    log.dirs=/var/lib/kafka/data
    log.retention.hours=168
    log.segment.bytes=1073741824
    log.retention.check.interval.ms=300000
    
    # Topic configuration
    num.partitions=3
    default.replication.factor=1
    offsets.topic.replication.factor=1
    transaction.state.log.replication.factor=1
    transaction.state.log.min.isr=1
    
    # Auto topic creation
    auto.create.topics.enable=true
    
    # KRaft metadata configuration
    metadata.log.segment.bytes=1073741824
    metadata.max.retention.bytes=1073741824
    
    # Performance tuning
    num.network.threads=3
    num.io.threads=8
    socket.send.buffer.bytes=102400
    socket.receive.buffer.bytes=102400
    socket.request.max.bytes=104857600
    
    # Log cleanup
    log.cleanup.policy=delete
    log.cleaner.enable=true
    log.cleaner.threads=2
    log.cleaner.io.max.bytes.per.second=1048576
